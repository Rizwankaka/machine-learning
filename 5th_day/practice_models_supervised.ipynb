{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the best model with best hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# train test split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# import regression algorithm \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "# Evaluate the models \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# import grid search cv for cross validation \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# import preprocessors\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset \n",
    "df= sns.load_dataset('tips')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and variables\n",
    "X=df.drop('tip', axis=1)\n",
    "y=df['tip']\n",
    "\n",
    "# label encode categorical variables \n",
    "le= LabelEncoder()\n",
    "X['sex']= le.fit_transform(X['sex'])\n",
    "X['smoker']= le.fit_transform(X['smoker'])\n",
    "X['day']= le.fit_transform(X['day'])\n",
    "X['time']= le.fit_transform(X['time'])\n",
    "\n",
    "# split the data into train and test data with 80% training dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of the list of models to evaluate the performance \n",
    "models = { \n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Support Vector Regression': SVR(),\n",
    "        'Decision Tree Regression': DecisionTreeRegressor(),\n",
    "        'Random Forest Regression': RandomForestRegressor(),\n",
    "        'Gradient Boosting Regression': GradientBoostingRegressor(),\n",
    "        'KNN Regression': KNeighborsRegressor(),\n",
    "        'XGBoost Regression': XGBRegressor()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 1825912.9915253515\n",
      "Linear Regression MAE: 858.7084697710096\n",
      "Linear Regression RMSE: 1351.2634796831267\n",
      "Linear Regression R2: 0.885139743367963\n",
      "Support Vector Regression MSE: 17791095.454944562\n",
      "Support Vector Regression MAE: 2747.3256009251854\n",
      "Support Vector Regression RMSE: 4217.949200137972\n",
      "Support Vector Regression R2: -0.11916055102542833\n",
      "Decision Tree Regression MSE: 520135.9549267705\n",
      "Decision Tree Regression MAE: 351.7402206154987\n",
      "Decision Tree Regression RMSE: 721.2045167126801\n",
      "Decision Tree Regression R2: 0.9672805059475864\n",
      "Random Forest Regression MSE: 295361.9737615915\n",
      "Random Forest Regression MAE: 267.9123321933983\n",
      "Random Forest Regression RMSE: 543.4721462610494\n",
      "Random Forest Regression R2: 0.9814200609431776\n",
      "Gradient Boosting Regression MSE: 430067.88364194834\n",
      "Gradient Boosting Regression MAE: 364.9299448070813\n",
      "Gradient Boosting Regression RMSE: 655.7956111792365\n",
      "Gradient Boosting Regression R2: 0.9729462971600609\n",
      "KNN Regression MSE: 795139.3987764182\n",
      "KNN Regression MAE: 480.76881720430106\n",
      "KNN Regression RMSE: 891.7058925320715\n",
      "KNN Regression R2: 0.9499812335934984\n",
      "XGBoost Regression MSE: 289592.469944461\n",
      "XGBoost Regression MAE: 275.0341176339596\n",
      "XGBoost Regression RMSE: 538.1379655297153\n",
      "XGBoost Regression R2: 0.981782995372228\n"
     ]
    }
   ],
   "source": [
    "# train and predict each model with evaluation metrics as well making a for loop to iterate over the models \n",
    "for name, model in models.items():\n",
    "    # fit each model from models on training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make prediction from each model \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "   # MSE for each model \n",
    "    print(name, 'MSE:', mean_squared_error(y_test, y_pred)) \n",
    "    \n",
    "    # MAE for each model\n",
    "    print(name, 'MAE:', mean_absolute_error(y_test, y_pred))\n",
    "    \n",
    "    # RMSE for each model\n",
    "    print(name, 'RMSE:', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    # R2 score for each model\n",
    "    print(name, 'R2:', r2_score(y_test, y_pred))\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.items()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignmnet: find the best model based on each metrics from above mentioned Results?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of list of models to evaluate performance with hyperparameters\n",
    "models = { \n",
    "        'Linear Regression': (LinearRegression(), {}),\n",
    "        'Support Vector Regression': (SVR(), {'kernel': ['rbf','poly','sigmoid'], 'C':[0.1, 1, 10], 'gamma':[1,0.1,0.01],'epsilon':[0.1,0.01, 0.001]}),\n",
    "        'Decision Tree Regression': (DecisionTreeRegressor(), {'max_depth':[None, 5, 10], 'splitter':['best', 'random']}),\n",
    "        'Random Forest Regression': (RandomForestRegressor(),{'n_estimators':[10, 100, 1000], 'max_depth':[None, 5, 10]}),\n",
    "        'Gradient Boosting Regression': (GradientBoostingRegressor(), {'loss':['ls', 'lad', 'hubber', 'quantile'], 'n_estimators':[10, 100, 1000]}),\n",
    "        'KNN Regression': (KNeighborsRegressor(), {'n_neighbors':np.arange(3,100, 3), 'weights':['uniform', 'distance']}),\n",
    "        'XGBoost Regression': (XGBRegressor(), {'n_estimators':[10, 100, 1000], 'learning_rate':[0.1, 0.01, 0.001]}),\n",
    "        }\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "    \n",
    "    # Create a pipeline \n",
    "    pipeline=GridSearchCV(model, params, cv=5)\n",
    "\n",
    "    #fit the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # make prediction from each model \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # print the performance metrics\n",
    "    print(name, 'MSE:', mean_squared_error(y_test, y_pred))\n",
    "    print(name, 'MAE:', mean_absolute_error(y_test, y_pred))\n",
    "    print(name, 'R2:', r2_score(y_test, y_pred))\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Preprocessor inside the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a preprocessor \n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=['numeric_scaling', StandardScaler(), ['total_bill', 'size']],remainder='passthrough')\n",
    "\n",
    "models = { \n",
    "        'Linear Regression': (LinearRegression(), {}),\n",
    "        'Support Vector Regression': (SVR(), {'kernel': ['rbf','poly','sigmoid'], 'C':[0.1, 1, 10], 'gamma':[1,0.1,0.01],'epsilon':[0.1,0.01, 0.001]}),\n",
    "        'Decision Tree Regression': (DecisionTreeRegressor(), {'max_depth':[None, 5, 10], 'splitter':['best', 'random']}),\n",
    "        'Random Forest Regression': (RandomForestRegressor(),{'n_estimators':[10, 100, 1000], 'max_depth':[None, 5, 10]}),\n",
    "        'Gradient Boosting Regression': (GradientBoostingRegressor(), {'loss':['ls', 'lad', 'hubber', 'quantile'], 'n_estimators':[10, 100, 1000]}),\n",
    "        'KNN Regression': (KNeighborsRegressor(), {'n_neighbors':np.arange(3,100, 3), 'weights':['uniform', 'distance']}),\n",
    "        'XGBoost Regression': (XGBRegressor(), {'n_estimators':[10, 100, 1000], 'learning_rate':[0.1, 0.01, 0.001]}),\n",
    "        }\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "    # create a pipeline with preprocessor\n",
    "    pipeline=Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "\n",
    "    # make a grid search cv to tune the hyperparameter\n",
    "    grid_search = GridSearchCV(pipeline, params, cv=5)\n",
    "\n",
    "    # fit the pipeline\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # make prediction from each model\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    # print the performance metrics\n",
    "    print(name, 'MSE:', mean_squared_error(y_test, y_pred))\n",
    "    print(name, 'MAE:', mean_absolute_error(y_test, y_pred))\n",
    "    print(name, 'R2:', r2_score(y_test, y_pred))\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tesla Laptops\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Tesla Laptops\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Tesla Laptops\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Logistic Regression\n",
      "Mean Accuracy: 0.9733333333333334\n",
      "\n",
      "Classifier: Decision Tree Classifier\n",
      "Mean Accuracy: 0.9600000000000002\n",
      "\n",
      "Classifier: Random Forest Classifier\n",
      "Mean Accuracy: 0.9600000000000002\n",
      "\n",
      "Classifier: KNN Classifier\n",
      "Mean Accuracy: 0.9733333333333334\n",
      "\n",
      "Classifier: SVM Classifier\n",
      "Mean Accuracy: 0.9666666666666668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create a dictionary of classifiers to evaluate \n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree Classifier': DecisionTreeClassifier(),\n",
    "    'Random Forest Classifier': RandomForestClassifier(),\n",
    "    'KNN Classifier': KNeighborsClassifier(),\n",
    "    'SVM Classifier': SVC()\n",
    "    }\n",
    "# perform kfold cross-validation and calculate the mean accuracy \n",
    "kfold=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for name, classifier in classifiers.items():\n",
    "    scores = cross_val_score(classifier, X, y, cv=kfold)\n",
    "    accuracy=np.mean(scores)\n",
    "    print('Classifier:', name)\n",
    "    print('Mean Accuracy:', accuracy)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
