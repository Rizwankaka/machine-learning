{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipeline in Machine Learning** \n",
    "\n",
    "In machine learning, a pipeline is a sequence of data processing steps that are chained together to automate and streamline the machine learning workflow. A pipeline allows you to combine multiple data processing and model training steps into a single object, making it easier to organize and manage your machine learning code. \n",
    "\n",
    "## Summary:\n",
    "\n",
    "Overall, piplelines are a powerful tool for managing and automating the machine learning workflow, promoting code reusability, consistency and efficiency. They help streamline the development and deployment of machine learning models, making it easier to iterate and experiment with different approaches. \n",
    "\n",
    "Here's an example of using a pipeline on the Titanic dataset to preprocess the data, train a model and make predictions: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7932960893854749\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the titanic dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# Select the features and target variables\n",
    "X = df[['pclass', 'sex', 'age', 'embarked']]\n",
    "y = df['survived']\n",
    "\n",
    "# Split the data into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore')), \n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipleline on the training data \n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy= accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation: \n",
    "\n",
    "In this example, we start by loading the Titanic dataset from seaborn using sns.load_dataset('titanic'). We then select the relevent features and target variable (survived) to train our model. Next, we split the data into training and test sets using train_test_split from scikit-learn. \n",
    "\n",
    "The pipeline is created using the Pipeline class from scikit-learn. It consists of three steps:\n",
    "\n",
    "1. Data preprocessing step: The SimpleImputer is used to handle missing values by replacing them with the most frequent value in each column. \n",
    "   \n",
    "2. Feature encoding step: The OneHotEncoder is used to encode categorical variables (sex and embarked) as binary features. \n",
    "\n",
    "3. Model training step: The RandomForestClassifier is used as the machine learning model for classification. \n",
    "\n",
    "We then fit the pipeline on the training data using pipeline.fit(X_train, y_train). Afterward, we make predictions on the test data using pipeline.predict(X_test)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyperparamater tuning in pipeline**\n",
    "\n",
    "Hyperparameter tuning in a pipeline involves optimizing the hyperparameters of the different steps in the pipeline to find the best combination that maximizes the model's performance. Here's and example of hyperparamter tuning in a pipeline and selecting the best model on the Titanic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7988826815642458\n",
      "Best Hyperparameters: {'model__max_depth': 10, 'model__min_samples_split': 2, 'model__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the titanic dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# Select the features and target variables\n",
    "X = df[['pclass', 'sex', 'age', 'embarked']]\n",
    "y = df['survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the hyperparamters to tune \n",
    "hyperparameters = {\n",
    "    'model__n_estimators':[100, 200, 300],\n",
    "    'model__max_depth':[None, 5, 10],\n",
    "    'model__min_samples_split':[2, 5, 10]\n",
    "}\n",
    "# Perform grid search cross-validation \n",
    "grid_search = GridSearchCV(pipeline, hyperparameters, cv=5)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model \n",
    "best_model= grid_search.best_estimator_\n",
    "\n",
    "# Make prediction on the test data using the best model \n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Print the best hyperparameters \n",
    "print('Best Hyperparameters:', grid_search.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Selecting best model in Pipeline**\n",
    "\n",
    "To select the best model when using multiple models in a pipeline, you can use techniques like cross-validation and evaluation metrics to compare their performance. Here's an example of how to accomplish this on Titanic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Cross-validation Accuracy: 0.7963656062247612\n",
      "Test Accuracy: 0.7932960893854749\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Cross-validation Accuracy: 0.8062247611543386\n",
      "Test Accuracy: 0.8044692737430168\n",
      "\n",
      "Best Model: Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
      "                ('model', GradientBoostingClassifier(random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Titanic dataset from seaborn \n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# Select the features and target variables\n",
    "X = df[['pclass', 'sex', 'age', 'embarked']]\n",
    "y = df['survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a list of models to evaluate \n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Selecting the best initial values for models \n",
    "best_model = None \n",
    "best_accuracy=0.0\n",
    "\n",
    "# Iterate over the models and evaluate their performance \n",
    "for name, model in models:\n",
    "    # Create a pipeline for each model \n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('model', model)\n",
    "\n",
    "    ])\n",
    "    # Perform cross-validation \n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5)  \n",
    "\n",
    "    # Get the average accuracy\n",
    "    mean_accuracy = scores.mean()\n",
    "\n",
    "    # Fit the pipeline on the training data \n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Print the performance metrics \n",
    "    print('Model:', name)\n",
    "    print('Cross-validation Accuracy:', mean_accuracy)\n",
    "    print('Test Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    # Check if the current model has the best accuracy\n",
    "    if accuracy> best_accuracy:\n",
    "        best_accuracy=accuracy\n",
    "        best_model=pipeline\n",
    "\n",
    "# Retrive the best model \n",
    "print ('Best Model:', best_model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Add more models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Cross-validation Accuracy: 0.7963656062247612\n",
      "Test Accuracy: 0.7932960893854749\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Cross-validation Accuracy: 0.8062247611543386\n",
      "Test Accuracy: 0.8044692737430168\n",
      "\n",
      "Model: Support Vector Classifier\n",
      "Cross-validation Accuracy: 0.8146163695459471\n",
      "Test Accuracy: 0.7821229050279329\n",
      "\n",
      "Model: Logistic Regression\n",
      "Cross-validation Accuracy: 0.7823106470993795\n",
      "Test Accuracy: 0.776536312849162\n",
      "\n",
      "Best Model: Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
      "                ('model', GradientBoostingClassifier(random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Titanic dataset from seaborn \n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# Select the features and target variables\n",
    "X = df[['pclass', 'sex', 'age', 'embarked']]\n",
    "y = df['survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a list of models to evaluate \n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('Support Vector Classifier', SVC(random_state=42)),\n",
    "    ('Logistic Regression', LogisticRegression(random_state=42)),\n",
    "]\n",
    "\n",
    "# Selecting the best initial values for models \n",
    "best_model = None \n",
    "best_accuracy=0.0\n",
    "\n",
    "# Iterate over the models and evaluate their performance \n",
    "for name, model in models:\n",
    "    # Create a pipeline for each model \n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('model', model)\n",
    "\n",
    "    ])\n",
    "    # Perform cross-validation \n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5)  \n",
    "\n",
    "    # Get the average accuracy\n",
    "    mean_accuracy = scores.mean()\n",
    "\n",
    "    # Fit the pipeline on the training data \n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Print the performance metrics \n",
    "    print('Model:', name)\n",
    "    print('Cross-validation Accuracy:', mean_accuracy)\n",
    "    print('Test Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    # Check if the current model has the best accuracy\n",
    "    if accuracy> best_accuracy:\n",
    "        best_accuracy=accuracy\n",
    "        best_model=pipeline\n",
    "\n",
    "# Retrive the best model \n",
    "print ('Best Model:', best_model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make practice of for loop? experts uses it and it differentiate from intermediate to proffessionals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
